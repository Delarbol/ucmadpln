{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de lenguaje natural\n",
    "\n",
    "*Carlos Isaac Zainea Maya* \n",
    "\n",
    "**Clase 2**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones elementales\n",
    "\n",
    "Python es un lenguaje de programación que ha permeado muchas ramas del conocimiento, la industria y la empresa. En el caso del procesamientro de textos es uno de los más fáciles y flexibles de trabajar. Empecemos con un texto simple y veamos algunas funciones elementales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Texto1=\"\\r Donald Knuth:\\n \\n  \\r Se    le conoce principalmente por ser el autor de la obra The Art of Computer Programming (El arte de programar computadoras), una de las más respetadas referencias en el campo de las ciencias de la computación. Sentó las bases y dio nombre al análisis de algoritmos, y ha realizado numerosos aportes a varias ramas teóricas de la informática. Es el creador de TEX, del sistema de diseño de tipos METAFONT y del estilo de programación conocido como programación literaria (Literate programming). Knuth es conocido como el 'padre del análisis de algoritmos'.\\n \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Texto1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**split**\n",
    "\n",
    "`split()` es un comando que  corta textos con un delimitador dado, usualmente se usa como delimitador el espació: `\" \"` para obtener una lista de palabras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Texto1.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**strip**\n",
    "\n",
    "El anterior texto tiene caracteres de generación de espacios en blanco, tales como \\n y \\r.\n",
    "\n",
    "El comando `strip()` permite borrar esos comando auxiliares del texto al final e inicio del texto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Texto1.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upper y lower**\n",
    "\n",
    "\n",
    "En ocasiones tenemos un montón de palabras que significan lo mismo pero para la máquina son iguales por el uso de mayúsculas y minúsculas propias de cada uno de los lenguajes.\n",
    "\n",
    "`upper()` y  `lower()` resuelven este problema dejando todas las letras del texto en Mayúscula o minúsculas respectivamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Texto1.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Texto1.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Texto1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replace**\n",
    "\n",
    "El comando `replace` remplaza en el texto caracteres por otros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Texto1.replace(\"o\",\"a\") #Aquí cambiamos o's por a's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Texto1.replace(\"o\",\"\",10) #el valor despues de los caracteres a remplazar indica la cantidad de remplazos que se quieren hacer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Texto1.replace(\":\",\"\").replace(\",\",\"\").replace(\"(\",\"\").replace(\")\",\"\").replace(\".\",\"\").replace(\"'\",\"\").split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expresiones regulares\n",
    "\n",
    "Las expresiones regulares son secuencias de carcateres que permiten establecer patrones de búsqueda. Se utilizan en el procesamiento de texto para la identificación de palabras similares y en Python se utilizan llamando el paquete re."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(re)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diccionarios\n",
    "\n",
    "Un diccionario es una estructura de datos indexada por claves o metadatos que pueden ser de cualquier tipo. Se usan con distintos lenguajes y se caracterizan por la facilidad en la implementación de procesos que evitan en los programadores la preocupación de estructurar datos. \n",
    "\n",
    "Usando el texto definido arriba crearemos un diccionario que guarde la cantidad de palabras que aparecieron en el texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frecuencias={}\n",
    "for palabra in Texto1.split():\n",
    "    if palabra in frecuencias:\n",
    "        frecuencias[palabra]+=1\n",
    "    else:\n",
    "        frecuencias[palabra]=1\n",
    "print(frecuencias)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Texto1paradic=re.sub( \"[^A-Za-z0-9óáéíúñ]+\",' ',Texto1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frecuencias={}\n",
    "for palabra in Texto1paradic.lower().split():\n",
    "    if palabra in frecuencias:\n",
    "        frecuencias[palabra]+=1\n",
    "    else:\n",
    "        frecuencias[palabra]=1\n",
    "print(frecuencias) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algo de gráficos\n",
    "\n",
    "Veamos a continuación algunas gráficas sencillas para vsualizar la frecuencia textual en este pequeño parrafo, necesitaremos instalar dos paquetes:\n",
    "\n",
    "`wordcloud` y `stop-words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install wordcloud\n",
    "!pip install stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from urllib.request import urlopen\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "from stop_words import get_stop_words\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagramas de barras\n",
    "\n",
    "Iniciemos creando un diagrama de barras, observe que se transforma el diccionario en una lista para poder hacer el gráfico,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(range(len(frecuencias)), list(frecuencias.values()), align='center')\n",
    "plt.xticks(range(len(frecuencias)), list(frecuencias.keys()),rotation=90)\n",
    "plt.xlabel(\"Palabra\")\n",
    "plt.ylabel(\"Cantidad de palabras\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora hacemos el mismo ejercicio pero organizando los valores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "frecuencias_ordenadas=sorted(frecuencias.items(),key=operator.itemgetter(1),reverse=True)\n",
    "dictfrecord=dict((key,value) for key,value in frecuencias_ordenadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(range(len(dictfrecord)), list(dictfrecord.values()), align='center')\n",
    "plt.xticks(range(len(dictfrecord)), list(dictfrecord.keys()),rotation=90)\n",
    "plt.xlabel(\"Palabra\")\n",
    "plt.ylabel(\"Cantidad de palabras\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud\n",
    "\n",
    "Word cloud es un tipo de gráfico bien interesante que dibuja las palabras de acuerdo a su frecuencia, cuando aparece muchas veces una palabra la dibuja más grande. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(Texto1paradic.upper())\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=get_stop_words(\"es\")\n",
    "stop_words.extend(get_stop_words(\"en\"))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(stopwords=stop_words, max_font_size=50, max_words=100, background_color=\"white\").generate(Texto1paradic)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frecuencias={}\n",
    "for palabra in Texto1paradic.lower().split():\n",
    "    if palabra not in stop_words:\n",
    "        if palabra in frecuencias:\n",
    "            frecuencias[palabra]+=1\n",
    "        else:\n",
    "            frecuencias[palabra]=1\n",
    "print(frecuencias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "frecuencias_ordenadas=sorted(frecuencias.items(),key=operator.itemgetter(1),reverse=True)\n",
    "dictfrecord=dict((key,value) for key,value in frecuencias_ordenadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(range(len(dictfrecord)), list(dictfrecord.values()), align='center')\n",
    "plt.xticks(range(len(dictfrecord)), list(dictfrecord.keys()),rotation=90)\n",
    "plt.xlabel(\"Palabra\")\n",
    "plt.ylabel(\"Cantidad de palabras\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foto=urlopen(\"https://www.ucentral.edu.co/sites/default/files/inline-images/maest-analitica-datos_0.jpg\")\n",
    "fondo=np.array(Image.open(foto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(stopwords=stop_words, max_font_size=80, max_words=100, background_color=\"white\",mask=fondo).generate(Texto1paradic)\n",
    "image_colors = ImageColorGenerator(fondo)\n",
    "plt.figure(figsize=[7,7])\n",
    "plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de páginas web usando BeautifulSoup y NLTK\n",
    "\n",
    "[BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) es una biblioteca de Python para extraer datos de archivos HTML y XML. Funciona con su analizador favorito para proporcionar formas idiomáticas de navegar, buscar y modificar el árbol de análisis. Comúnmente ahorra a los programadores horas o días de trabajo.\n",
    "\n",
    "[NLTK - Natural Language ToolKit -](https://www.nltk.org/) es una libreia imprescindible en el lenguaje natural para Python. Cuenta con más de 50 corpus y recursos léxicos como WordNet, junto con un conjunto de bibliotecas de procesamiento de texto para clasificación, tokenización, derivación, etiquetado, análisis y razonamiento semántico, envoltorios para bibliotecas de PNL de potencia industrial y un foro de discusión activo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagina=urlopen(\"https://www.ucentral.edu.co/programa-academico/maestria-analitica-datos\")\n",
    "CodHTML=pagina.read().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CodHTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limpieza simple usando expresiones regulares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " CodHTMLclean1=re.split('\\W+',CodHTML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " CodHTMLclean1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limpieza usando BeautifulSoup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "CodHTMLclean=BeautifulSoup(CodHTML).get_text()\n",
    "tokens = nltk.word_tokenize(CodHTMLclean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.index(\"Maestría\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.pop(334)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.index(\"Maestría\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.pop(1625)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.index(\"Maestría\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[2092:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "Freq_dist_nltk=nltk.FreqDist(tokens[2091:4511])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Freq_dist_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Freq_dist_nltk.plot(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=get_stop_words(\"es\")\n",
    "tokensclean=[pal for pal in tokens[2091:4511] if len(pal.lower())>1 and (pal.lower() not in stop_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokensclean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Freq_dist=nltk.FreqDist(tokensclean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Freq_dist.plot(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minuscularizar= lambda x: x.lower()\n",
    "Freq_dist2=nltk.FreqDist(list(map(minuscularizar,tokensclean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Freq_dist2.plot(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(\" \".join(tokensclean))\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2=\"Python can be easy to pick up whether you're a first time programmer or you're experienced with other languages. The following pages are a useful first step to get on your way writing programs with Python!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "tokens = nltk.word_tokenize(text2)\n",
    "tagged=nltk.pos_tag(tokens,lang=\"eng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(nltk.pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities=nltk.chunk.ne_chunk(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"omw\")\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "poses={'n':'sustantivo','v':'verbo','s':'adj(s)','a':'adjetivo','r':'adverbio'}\n",
    "for synset in wn.synsets(\"amigo\",lang=\"spa\"):\n",
    "    print(\"{}:{}\".format(poses[synset.pos()],\", \".join([l.name() for l in synset.lemmas(\"spa\")])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "poses={'n':'sustantivo','v':'verbo','s':'adj(s)','a':'adjetivo','r':'adverbio'}\n",
    "for synset in wn.synsets(\"friend\"):\n",
    "    print(\"{}:{}\".format(poses[synset.pos()],\", \".join([l.name() for l in synset.lemmas()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "poses={'n':'sustantivo','v':'verbo','s':'adj(s)','a':'adjetivo','r':'adverbio'}\n",
    "for synset in wn.synsets(\"amigo\",lang=\"spa\"):\n",
    "    print(\"{}:{}\".format(poses[synset.pos()],\", \".join([translator.translate(l.name(),dest='es').text for l in synset.lemmas()])))\n",
    "    print(synset.lemmas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(translator.translate('fan',dest='es'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "poses={'n':'sustantivo','v':'verbo','s':'adj(s)','a':'adjetivo','r':'adverbio'}\n",
    "for synset in wn.synsets(\"perfecto\",lang=\"spa\"):\n",
    "    print(\"{}:{}\".format(poses[synset.pos()],\", \".join([l.name() for l in synset.lemmas(\"spa\")])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "panda=wn.synset(\"dog.n.01\")\n",
    "hyper= lambda s:s.hypernyms()\n",
    "list(panda.closure(hyper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
